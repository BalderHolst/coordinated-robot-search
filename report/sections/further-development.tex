\section{Further Development}
While this project demonstrates a complete simulation-based framework for coordinated robotic search, several opportunities exist for real-world deployment and further algorithmic enhancements. This section outlines key considerations for transitioning to physical systems and explores future directions in algorithmic development.

\subsection{Real World Implementation}
Although this project has not been tested on physical hardware, the entire software stack has been developed with real-world deployment in mind. Both the simulation architecture and behavior algorithms are modular, portable, and compatible with widely available robotics platforms and middleware.

To ensure reliable performance in real-world conditions, the following aspects should be evaluated and potentially refined:

\begin{itemize}
  \item \textbf{Sensor calibration and frame alignment}: Accurate transformations between the LiDAR, camera, and base frames are essential for sensor fusion and localization.
  \item \textbf{Robust object detection}: The current color-based detection may perform poorly under varying lighting conditions. More advanced detection techniques (e.g., learned models or depth-based filtering) could improve reliability.
  \item \textbf{Communication robustness}: Real-world wireless communication introduces latency, packet loss, and range limitations. Fault-tolerant strategies such as message queuing, compression, or redundancy may be necessary.
  \item \textbf{Real-time performance}: The behavior logic and path planning algorithms should be profiled on embedded hardware to ensure real-time feasibility.
  \item \textbf{ROS 2 control integration}: Currently, differential drive is simulated via Gazebo plugins. Transitioning to \texttt{ros2\_control} would enable consistent controller behavior on both simulated and physical robots.
\end{itemize}

A practical next step would be to implement a small-scale multi-robot testbed using TurtleBot 4 platforms. This would facilitate hardware-in-the-loop testing and help uncover system-level challenges not observable in simulation.

\subsubsection{Hardware Requirements}
The project targets differential drive robots equipped with:
\begin{itemize}
  \item A 2D LiDAR scanner.
  \item An RGB camera with a known field of view.
  \item Wheel odometry or an IMU for dead reckoning.
  \item A communication module (e.g., Wi-Fi, Zigbee, or LoRa).
\end{itemize}
Although TurtleBot 4 is used in simulation, the software can be adapted to other platforms by adjusting sensor interfaces and configuration parameters.

\subsubsection{Using ROS 2}
Deployment is primarily managed through a dedicated ROS 2 node that subscribes to standard input topics:
\begin{itemize}
  \item \texttt{/scan}: LiDAR data for localization and obstacle detection.
  \item \texttt{/camera/image}: RGB image stream for object detection.
  \item \texttt{/amcl\_pose}: Global pose estimated via AMCL.
\end{itemize}

Output is published to \texttt{/cmd\_vel}, which controls linear and angular velocities. In simulation, Gazebo handles differential drive via plugins. For physical robots, \texttt{ros2\_control} \cite{ros2-control} is recommended. It offers modular hardware interfaces and is compatible with both real and simulated environments via a Gazebo plugin.

\subsubsection{Using the Rust Library}
For systems that do not support ROS 2, the behavior algorithms can be used via the standalone \texttt{botbrain} Rust library. This interface allows for integration in embedded or custom platforms by directly feeding sensor data to \texttt{botbrain}, receiving control outputs and handling communication between robots. This modular design is critical in swarm systems, where individual robots should be cheep and replalable.

\subsubsection{Communication Considerations}\label{sub:communication}
In simulation, communication is assumed to be instantaneous and lossless. In real-world settings, this is not the case. Communication technologies under consideration include:

\begin{itemize}
  \item \textbf{Wi-Fi}: High bandwidth but susceptible to interference and limited range.
  \item \textbf{Zigbee}: Suitable for low-bandwidth mesh networks but limited in data rate.
  \item \textbf{LoRa}: Extremely long range but low throughput; suitable for infrequent updates.
\end{itemize}

The system assumes a broadcast model where each robot shares observations over a shared channel. Messages are already serialized in a compact binary format. If communication becomes a bottleneck, strategies such as message prioritization, differential updates, or lossy compression may be necessary.

\subsection{Advancing Deep Reinforcement Learning}
The deep reinforcement learning component can be extended in multiple directions to improve both learning performance and real-world applicability:

\begin{itemize}
  \item \textbf{Network architecture tuning}: Adjusting the number of layers, neurons, and activation functions to improve learning stability and performance.
  \item \textbf{Hyperparameter optimization}: Fine-tuning learning rate, discount factor, batch size, and exploration parameters for better convergence.
  \item \textbf{Reward function design}: Constructing effective reward signals that balance exploration, efficiency, and safety.
  \item \textbf{Reward shaping}: Incorporating additional reward terms, such as for maintaining communication, minimizing energy use, or avoiding collisions.
  \item \textbf{Domain randomization}: Introducing variability in sensor noise, dynamics, and environment layouts during training to improve generalization.
  \item \textbf{Curriculum learning}: Training on progressively more complex environments to facilitate gradual policy improvement.
  \item \textbf{Transfer learning}: Adapting a pretrained simulation policy to real-world conditions via fine-tuning on physical robots.
\end{itemize}

Incorporating these techniques could help develop policies that are not only more effective but also more transferable and robust under real-world deployment conditions.

\subsubsection{Neuroevolution of Augmenting Topologies (NEAT)}
Designing an optimal neural network architecture for a DQN agent remains a non-trivial task. The architecture can greatly affect both convergence speed and final performance.\\

Neuroevolution of Augmenting Topologies (NEAT) \cite{neat} is a genetic algorithm designed to evolve both the topology and weights of neural networks. Each candidate solution, or genome, is represented as a graph of neurons and connections. Networks are evaluated based on a fitness function, and mutation/crossover operations allow the population to explore a diverse architecture space.\\

NEAT offers the advantage of automatic architecture discovery, potentially outperforming manually designed networks. However, it is computationally intensive and typically requires many training cycles.

A hybrid approach could involve using NEAT to discover promising network architectures, which are then fine-tuned using standard DQN training. This could accelerate learning and yield more effective policies.

\subsubsection{Proximal Policy Optimization (PPO)}
Another promising direction for advancing the reinforcement learning component is the use of Proximal Policy Optimization (PPO) \cite{ppo}. PPO is a widely used on-policy reinforcement learning algorithm that has demonstrated strong empirical performance across a range of robotic control tasks.

Unlike DQN, which is value-based and limited to discrete action spaces, PPO belongs to the family of policy gradient methods and naturally supports continuous action spaces. This makes it particularly well-suited for real-world robotics applications, where actions such as velocity commands are typically continuous.

PPO improves training stability by preventing large updates to the policy. It achieves this by clipping the policy objective function, ensuring that new policies do not deviate too far from the current one. This balances the trade-off between exploration and exploitation while maintaining sample efficiency.

Integrating PPO into the current framework would enable:
\begin{itemize}
  \item Learning of smoother, continuous control policies.
  \item Better handling of stochastic environments and sensor noise.
  \item More natural extension to multi-agent reinforcement learning setups.
\end{itemize}

To support PPO, the environment interface must provide not only scalar rewards but also policy gradients and advantage estimates, which can be implemented through Generalized Advantage Estimation (GAE). Furthermore, parallelized simulation environments would significantly accelerate data collection and improve sample efficiency.

Overall, PPO offers a robust and scalable alternative to DQN for learning complex behaviors in dynamic environments. Exploring this method could enhance policy quality and ease the transition to real-world deployment, especially in continuous control domains.
