\section{Deep Reinforcement Learning}
\label{sec:rl}
In contrast to the manually developed algorithms, a Deep Q-learning approach as also explored. Reinforcement learning suits this problem well, as coverage can serve as a reward function. These types of systems are also very adaptable to new environments as they can be retrained on new data to expand and tune their capabilities. \\

Traditional Q-learning methods store a table of discrete state-action pairs and their expected reward in memory {\color{red}[SOURCE]}, which can be cumbersome in real world applications where state is often continuous. Another pitfall of traditional Q-learning is that the size of the Q-table grows with the state space. Neural networks solve both of these problems. The Q-table can be approximated by using a neural network, then used to pick an action given a state. These networks are called a DQNs (Deep Q Networks). \\

Designing a neural network involves several important considerations, including selecting the number of layers and neurons, choosing appropriate activation functions, and determining key hyperparameters. After the architecture is defined, the network must be trained, evaluated, and iteratively optimized to ensure stable and effective learning. Finally, the trained model can be validated on unseen environments and integrated into the broader system for deployment.

% \subsection{Network}
% The general idea of a neural network is to construct a layered architecture where each layer, consists of multiple interconnected neurons. Each neuron applies a learned weight and bias to its inputs and passes the result through an activation function. The final output of the network determines the action that the agent should take based on the current input state.

\subsection{Learning Framework}
The framework for defining robots controlled by neural networks is implemented as a part of the \texttt{botbrain} library and can be enabled by the "rl" feature flag. The models are implemented using the \texttt{burn} \cite{burn} library, which is a Deep Learning Framework for Rust. \texttt{burn} provides a high-level API for building and training deep learning networks with multiple backends. \\

\texttt{botbrain} defines the \texttt{RlRobot} which is generic across four parameters: the \texttt{burn} backend, the state type which the robots provides as input to the network, the action type which defines the discrete actions the robot can take, and the network which defined how the input and output layers should be connected. Several state, action and network implementations are also provided, which can be combined in any combination to create specializations of \texttt{RlRobot}. This system facilitates composition and expansion the \texttt{RlRobot} components and makes it easy to swap out single elements and compare differing architectures.

% TODO: Maybe create a figure

% over the network architecture within \texttt{RlRobot} and makes is possible to compare robots.


% TODO: Write how our network and activation function works (maybe in each of the models or only one?)
% For this project, a feedforward fully connected neural network was used. The input to the network is a tensor representation part of the robotâ€™s state, including sensor readings, and proximity to other robots. 

% The output tensor represents descrete set of actions the robot can take. This is different values of steer angle and speed for the TurtleBot 4.

% The network also consists of two hidden layers, with varying sizes depending on the model, and uses the ReLU (Rectified Linear Unit) activation function for all hidden layers.

\subsection{Reward}
The reward function is a central component of reinforcement learning, as it defines the value of agent actions. A well designed reward function encourages desirable behavior and penalizes undesirable outcomes. \\

In this project, the reward is primarily based on map coverage. At each time step, a positive reward is given proportional to the amount of new area explored by the agent. This incentivizes the agent to move into unexplored regions of the map. Additionally, a small negative reward is applied for each step to encourage efficiency and to discourage the agent from idling or circling already explored areas. This negative also scales linearly with the time since new area was covered. A further negative reward is given when the robot collides a wall to promote safe navigation.
% TODO: Correct if we did not use negative reward for range
% TODO: Actually implement negative reward outside of range
To maintain network connectivity among robots, a penalty is also introduced if the agent moves outside of the communication range of the rest of the swarm. This helps keep the robots together to allow them to communicate.

\subsection{Training}
Training the neural network involves allowing the agent to interact with the environment over multiple episodes while incrementally improving its policy based on received rewards. The \texttt{trainer} is implemented as a seperate program and was inspired by the \texttt{burn-rl-examples} repository \cite{burn-rl-examples}. \\

Each training episode starts with one or more robots placed in a randomly generated environment, as described in the next subsection {\color{red}DESCRIBE?}. The episode proceeds step by step, during which the agent observes its current state, selects an action using an $\epsilon$-greedy policy, balancing exploration and exploitation, executes the action, and receives a reward. The transition tuple,  containing state, action, reward and next state, is stored in a replay buffer. \\

\subsubsection{Target and Policy Networks}
When training a DQN, two networks with identical structure are used called te target and policy networks. The policy network is encodes the current best policy and is updated each time step using gradient decent. The target network is used as a reference when calculating the loss. Each time step, the weights of the target network are updated in a small linear step towards the policy network. The size of the step is configured with the hyperparameter $\tau$. These networks both continually continually approach the optimal policy during training, the policy network approaches faster and is used to get on policy action and the target network trails behind, but provides more stable state action values.

\subsubsection{Calculating Loss}
Each time step, mini-batches of transitions are sampled randomly from the replay buffer and used to train the policy network by performing gradient decent on the loss function. Loss is calculated as the mean square error between the value of an action as determined by the policy network, and the expected value of the action. The expected value is the value of the best action from the next state as given by the target network plus the reward gotten from taking the current action. \\

Training continues for a fixed number of episodes or until the performance converges. An episode is terminated early if the agent fails to make significant progress (i.e., no new area is explored for a defined number of steps), or when the entire map is successfully covered.


\subsubsection{Generating Training Environments}
As to not overfit the network, agents must be trained on different environments. A simple program was created to generate worlds populated by line and circle obstacles. The generator is however quite simplistic and does not check that the entire map is reachable which could lead to problems during training. \Cref{fig:generated-enviornments} shows examples of generated environments. 

\def\w{0.31\textwidth}
\begin{figure}[H]
    \begin{subfigure}{\w}
        \makebox(\textwidth, \textwidth)[\textwidth]{
            \includegraphics[width=\linewidth]{figures/generated-worlds/world_0.png}
        }
    \end{subfigure}
    \hspace*{\fill}
    \begin{subfigure}{\w}
        \makebox(\textwidth, \textwidth)[\textwidth]{
            \includegraphics[width=\linewidth]{figures/generated-worlds/world_1.png}
        }
    \end{subfigure}
    \hspace*{\fill}
    \begin{subfigure}{\w}
        \makebox(\textwidth, \textwidth)[\textwidth]{
            \includegraphics[width=\linewidth]{figures/generated-worlds/world_2.png}
        }
    \end{subfigure}

    \vspace{4mm}

    \begin{subfigure}{\w}
        \makebox(\textwidth, \textwidth)[\textwidth]{
            \includegraphics[width=\linewidth]{figures/generated-worlds/world_3.png}
        }
    \end{subfigure}
    \hspace*{\fill}
    \begin{subfigure}{\w}
        \makebox(\textwidth, \textwidth)[\textwidth]{
            \includegraphics[width=\linewidth]{figures/generated-worlds/world_4.png}
        }
    \end{subfigure}
    \hspace*{\fill}
    \begin{subfigure}{\w}
        \makebox(\textwidth, \textwidth)[\textwidth]{
            \includegraphics[width=\linewidth]{figures/generated-worlds/world_5.png}
        }
    \end{subfigure}
    \caption{Examples of generated environments}
    \label{fig:generated-enviornments}
\end{figure}


% TODO: If better hardware how could we train it better?
% How is the params, (maybe better to look at than pure performance)
% Why is it not that good?
%   Leaning rate?
%   Cost function?
